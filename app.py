# app.py

import uuid
from typing import List, Tuple

import streamlit as st
from PyPDF2 import PdfReader

from rag import SimpleRAG
from openai_api import chat_completion

# -----------------------------
# 1) ê¸°ë³¸ ì„¤ì • & ì„¸ì…˜ ì´ˆê¸°í™”
# -----------------------------
st.set_page_config(page_title="My Local RAG Chatbot", layout="wide")

# ê°„ë‹¨ íŒ¨ìŠ¤ì›Œë“œ ìž ê¸ˆ (ì›í•˜ë©´ ì œê±° ê°€ëŠ¥)
PASSWORD = "demo"
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    password = st.text_input("Enter password to access:", type="password")
    if password != PASSWORD:
        st.stop()
    else:
        st.session_state.authenticated = True

# ì„¸ì…˜ ìƒíƒœ ê¸°ë³¸ê°’
if "model_temperature" not in st.session_state:
    st.session_state.model_temperature = 0.7
if "model_top_p" not in st.session_state:
    st.session_state.model_top_p = 1.0
if "model_max_tokens" not in st.session_state:
    st.session_state.model_max_tokens = 1024
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = (
        "You are a helpful, reliable, deeply knowledgeable AI assistant. "
        "Use the given context from documents when it is relevant. "
        "If the context is not relevant, ignore it."
    )
if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = {}
if "current_session" not in st.session_state:
    sid = str(uuid.uuid4())
    st.session_state.chat_sessions[sid] = []  # List[Tuple[role, content]]
    st.session_state.current_session = sid

# RAGìš© ê°ì²´
if "rag" not in st.session_state:
    st.session_state.rag = None  # type: ignore


# -----------------------------
# 2) ì‚¬ì´ë“œë°”: ì„¤ì • + ì„¸ì…˜ ê´€ë¦¬ + PDF ì—…ë¡œë“œ
# -----------------------------
with st.sidebar:
    st.header("ðŸ”§ Configuration (Local vLLM)")

    st.slider(
        "Temperature",
        0.0,
        1.0,
        key="model_temperature",
    )
    st.slider(
        "Top-p",
        0.1,
        1.0,
        key="model_top_p",
    )
    st.slider(
        "Max Tokens",
        256,
        4096,
        key="model_max_tokens",
    )

    st.text_area("System Prompt", key="system_prompt", height=120)
    st.download_button("ðŸ“¥ Export Prompt", st.session_state.system_prompt, file_name="prompt.txt")

    st.divider()
    st.subheader("ðŸ’¬ Chat Sessions")

    # ì„¸ì…˜ ëª©ë¡ ë²„íŠ¼
    for session_id in list(st.session_state.chat_sessions.keys()):
        label = f"ðŸ” {session_id[:8]}"
        if st.button(label, key=f"switch_{session_id}"):
            st.session_state.current_session = session_id

    if st.button("âž• New Session"):
        new_id = str(uuid.uuid4())
        st.session_state.chat_sessions[new_id] = []
        st.session_state.current_session = new_id

    st.divider()
    st.subheader("ðŸ“„ PDF for RAG")

    uploaded_file = st.file_uploader("Upload PDF", type=["pdf"])
    if uploaded_file is not None:
        # 1) PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        reader = PdfReader(uploaded_file)
        full_text = ""
        for page in reader.pages:
            full_text += (page.extract_text() or "") + "\n"

        # 2) ê°„ë‹¨í•œ chunking (ë¬¸ìž ìˆ˜ ê¸°ì¤€)
        def chunk_text(text: str, chunk_size: int = 500, overlap: int = 100) -> List[str]:
            text = text.replace("\r", " ").replace("\n", " ")
            tokens = list(text)
            chunks = []
            start = 0
            while start < len(tokens):
                end = start + chunk_size
                chunk = "".join(tokens[start:end]).strip()
                if chunk:
                    chunks.append(chunk)
                start = end - overlap
                if start < 0:
                    start = 0
            return chunks

        chunks = chunk_text(full_text, chunk_size=500, overlap=100)

        # 3) SimpleRAGì— ë¬¸ì„œ ì¶”ê°€
        rag = SimpleRAG()
        added = rag.add_documents(chunks)

        st.session_state.rag = rag
        st.success(f"âœ… PDF ë¡œë“œ ë° ìž„ë² ë”© ì™„ë£Œ! (ì¶”ê°€ëœ ì²­í¬ ìˆ˜: {added} ê°œ)")


# -----------------------------
# 3) ë©”ì¸ ì˜ì—­: ì±„íŒ… ì¸í„°íŽ˜ì´ìŠ¤
# -----------------------------
st.title("ðŸ¤– Local RAG Chatbot (vLLM)")

current_session_id = st.session_state.current_session
history: List[Tuple[str, str]] = st.session_state.chat_sessions[current_session_id]

# ê³¼ê±° ëŒ€í™” í‘œì‹œ
for role, msg in history:
    with st.chat_message(role):
        st.markdown(msg)

# ì‚¬ìš©ìž ìž…ë ¥
user_input = st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”...")

if user_input:
    # 1) ì‚¬ìš©ìž ë©”ì‹œì§€ í™”ë©´ & ì„¸ì…˜ì— ê¸°ë¡
    st.session_state.chat_sessions[current_session_id].append(("user", user_input))
    with st.chat_message("user"):
        st.markdown(user_input)

    # 2) (ì„ íƒ) RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context_text = ""
    if st.session_state.rag is not None:
        context_text = st.session_state.rag.get_context(user_input, k=3)

    # 3) vLLMì— ë³´ë‚¼ messages êµ¬ì„±
    messages = []

    # (1) ê¸°ë³¸ system í”„ë¡¬í”„íŠ¸
    messages.append({"role": "system", "content": st.session_state.system_prompt})

    # (2) RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³„ë„ì˜ system ë©”ì‹œì§€ë¡œ ì „ë‹¬
    if context_text:
        messages.append(
            {
                "role": "system",
                "content": (
                    "ë‹¤ìŒì€ ì‚¬ìš©ìžê°€ ì§ˆë¬¸í•  ë•Œ ì°¸ê³ í•´ì•¼ í•  ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ìž…ë‹ˆë‹¤. "
                    "ê´€ë ¨ ìžˆì„ ë•Œë§Œ í™œìš©í•˜ê³ , ê´€ë ¨ì´ ì—†ìœ¼ë©´ ë¬´ì‹œí•˜ì„¸ìš”.\n\n"
                    f"{context_text}"
                ),
            }
        )

    # (3) ê¸°ì¡´ ëŒ€í™” ížˆìŠ¤í† ë¦¬
    #    vLLMì´ ì „ì²´ ëŒ€í™” ë§¥ë½ì„ ì´í•´í•˜ë„ë¡ user/assistant ë©”ì‹œì§€ ëª¨ë‘ ì „ë‹¬
    for role, msg in st.session_state.chat_sessions[current_session_id]:
        messages.append({"role": role, "content": msg})

    # 4) vLLM í˜¸ì¶œ
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            try:
                reply = chat_completion(
                    messages=messages,
                    temperature=st.session_state.model_temperature,
                    top_p=st.session_state.model_top_p,
                    max_tokens=st.session_state.model_max_tokens,
                )
            except Exception as e:
                reply = f"âŒ vLLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

            st.markdown(reply)
            st.session_state.chat_sessions[current_session_id].append(("assistant", reply))
